\documentclass{report}

\input{preamble}
\input{macros}
\input{letterfonts}

\usepackage{tikz}
\usepackage{tikz-3dplot}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage{smartdiagram}
\usesmartdiagramlibrary{additions}
\usepackage{xcolor}
\usepackage{forest}
\usepgfplotslibrary{colormaps}
\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{polar}
\pgfplotsset{compat=newest}
\tikzset{>=latex}
\usepackage{siunitx}

\title{\Huge{Computational Linear Algebra}\\EK103}
\author{\huge{Giacomo Cappelletto}}
\date{21/1/2}


\begin{document}


\maketitle
\newpage
\pdfbookmark[section]{\texorpdfstring{\contentsname}{Contents}}{toc}
\tableofcontents
\pagebreak

\chapter{\texorpdfstring{Basics}{Basics}}

\section{\texorpdfstring{Vectors, Norms and Products}{Vectors, Norms and Products}}

\nt{
	Let us consider two vectors in \(\mathbb{R}^3\):
	\[
		u =
		\begin{pmatrix}
			1 \\
			1 \\
			1
		\end{pmatrix}
		\quad \text{and} \quad
		v =
		\begin{pmatrix}
			1  \\
			-1 \\
			1
		\end{pmatrix}.
	\]
	We wish to compute their magnitudes (norms and norm-squared), the angle between them, and the plane that they span. These methods are directly applicable to computational tools such as MATLAB.
}

\dfn{Norm of a Vector}{
	For a vector \(x = (x_1, x_2, \dots, x_n)\in \mathbb{R}^n\), its norm is
	\[
		\|x\| \;=\; \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}.
	\]
	In many programming languages (including MATLAB), this is computed via \texttt{norm(x)}, while the square of the norm is \(\|x\|^2 = x \cdot x = x_1^2 + \cdots + x_n^2\).

	Norm squared is the result of the dot product of a vector with itself. For example, the norm squared of \(x\) is \\
	\[
		\|x\|^2 \;=\; x \cdot x = x_1^2 + \cdots + x_n^2 = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} \cdot \begin{bmatrix} x_1 & \cdots & x_n \end{bmatrix} .
	\]
}

\ex{Norms and Norm-Squared of \(\,u\) and \(\,v\)}{
	\[
		\|u\| \;=\;\sqrt{1^2 + 1^2 + 1^2} \;=\;\sqrt{3},
		\quad
		\|v\| \;=\;\sqrt{1^2 + (-1)^2 + 1^2} \;=\;\sqrt{3}.
	\]
	Thus, both vectors have the same magnitude \(\sqrt{3}\). Their squared norms are
	\[
		\|u\|^2 \;=\;3,
		\quad
		\|v\|^2 \;=\;3.
	\]
	In MATLAB notation, one could write:
	\begin{itemize}
		\item \texttt{norm(u)} or \texttt{norm(u,2)} for the norm of \(u\).
		\item \texttt{dot(u,u)} or \texttt{norm(u)\^{}2} for \(\|u\|^2\).
	\end{itemize}
}


\dfn{Angle Between Two Vectors}{
	The angle \(\theta\) between two nonzero vectors \(u\) and \(v\) in \(\mathbb{R}^n\) is given by
	\[
		\theta \;=\; \arccos \Bigl(\frac{u \cdot v}{\|u\|\|v\|}\Bigr).
	\]
}

\ex{Angle Between \(\,u\) and \(\,v\)}{
	First, compute the dot product:
	\[
		u \cdot v
		\;=\;
		(1)(1) + (1)(-1) + (1)(1)
		\;=\;
		1 - 1 + 1
		\;=\;
		1.
	\]
	Hence,
	\[
		\theta
		\;=\;
		\arccos \Bigl(\frac{u \cdot v}{\|u\|\|v\|}\Bigr)
		\;=\;
		\arccos\Bigl(\frac{1}{\sqrt{3}\,\sqrt{3}}\Bigr)
		\;=\;
		\arccos\Bigl(\tfrac{1}{3}\Bigr).
	\]
	In MATLAB, one could write:
	\[
		\texttt{theta = acos(dot(u,v)/(norm(u)*norm(v)));}
	\]
}

\dfn{Plane Spanned by Two Vectors}{
	The plane containing vectors \(u\) and \(v\) and passing through the origin is given by
	\[
		\{\;\alpha\,u + \beta\,v \;\mid\; \alpha, \beta \in \mathbb{R}\}.
	\]
	An equivalent description is all points \(x\in \mathbb{R}^3\) such that \(x \cdot (u \times v) = 0\).
}

\ex{Plane Containing \(\,u\) and \(\,v\)}{
	\begin{itemize}
		\item
		      \emph{Span form:}
		      \[
			      \text{Plane} = \bigl\{\,\alpha \begin{pmatrix}1\\1\\1\end{pmatrix}
			      \;+\;\beta \begin{pmatrix}1\\-1\\1\end{pmatrix}
			      \;\mid\; \alpha,\beta \in \mathbb{R}\bigr\}.
		      \]
		\item
		      \emph{Normal form:}
		      The cross product
		      \[
			      u \times v
			      =
			      \begin{vmatrix}
				      \mathbf{i} & \mathbf{j} & \mathbf{k} \\
				      1          & 1          & 1          \\
				      1          & -1         & 1
			      \end{vmatrix}
			      =
			      (2,\,0,\,-2).
		      \]
		      Hence, the plane also can be described by the set of points \(x = (x_1,x_2,x_3)\) for which
		      \[
			      (2,\,0,\,-2)\cdot (x_1,x_2,x_3) = 0
			      \quad\Longrightarrow\quad
			      2\,x_1 - 2\,x_3 = 0
			      \quad\Longrightarrow\quad
			      x_1 = x_3.
		      \]
	\end{itemize}
	In many computational environments, one simply keeps the span form or uses a symbolic package to compute the cross product and normal equation.
}

\dfn{Cross Product}{
	Construct a system of linear equations where the dot product of the vector is orthogonal to both the vectors in the matrix.
	\[
		\begin{bmatrix}
			u_1 & u_2 & u_3 \\
			v_1 & v_2 & v_3
		\end{bmatrix}
		\cdot
		\begin{bmatrix}
			x_1 \\
			x_2 \\
			x_3
		\end{bmatrix}
		=
		\begin{bmatrix}
			0 \\
			0
		\end{bmatrix}
	\]

}

\ex{Finding the Plane Spanned by Two Vectors}{
	\[
		\begin{bmatrix}
			1 & 1  & 1 \\
			1 & -1 & 1
		\end{bmatrix}
		\cdot
		\begin{bmatrix}
			x_1 \\
			x_2 \\
			x_3
		\end{bmatrix}
		=
		\begin{bmatrix}
			0 \\
			0
		\end{bmatrix}
	\]
	\[
		\begin{bmatrix}
			x_1 \\
			x_2 \\
			x_3
		\end{bmatrix}
		= t
		\begin{bmatrix}
			1 \\
			0 \\
			-1
		\end{bmatrix}
	\]

	Then, we must formulate an equation for any vector perpendicular to the normal of the plane, i.e. the cross product of the two original vectors.
	\[
		\begin{bmatrix}
			y_1 \\
			y_2 \\
			y_3
		\end{bmatrix}
		\cdot
		\begin{bmatrix}
			1 \\
			0 \\
			-1
		\end{bmatrix}
		=
		0
	\]
	Hence,
	\[
		y_1 - y_3 = 0
	\]
}

\dfn{Dot product}{

	We can take a vector $\vec{v}$ in $\mathbb{R}^n$ and a vector $\vec{w}$ in $\mathbb{R}^n$. Then, the dot product of $\vec{v}$ and $\vec{w}$ is defined as
	\[
		\vec{v} \cdot \vec{w} = \vec{v}^T \vec{w} = \vec{w}^T \vec{v}
	\]
	Therefore
	\[
		\norm{\vec{v}}^2 = \vec{v}^T \vec{v}
	\]

}

\dfn{Scalar Multiplication}{
	Scalar multiplication is the operation of multiplying a vector by a scalar. The result is a new vector with the same direction as the original vector, but with a magnitude that is the product of the original magnitude and the scalar.
	\[
		t \cdot \vec{v} = t \cdot \begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{bmatrix} = \begin{bmatrix} t\,a_1 \\ t\,a_2 \\ \vdots \\ t\,a_n \end{bmatrix}
	\]
	Where
	\[
		\norm{t \cdot \vec{v}} = \norm{\vec{v}} \cdot t
	\]
}

\dfn{Vector Addition}{
	Vector addition is the operation of adding two vectors together. The result is a new vector that is the sum of the two original vectors.
	\[
		\vec{v} + \vec{w} = \begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{bmatrix} + \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix} = \begin{bmatrix} a_1 + b_1 \\ a_2 + b_2 \\ \vdots \\ a_n + b_n \end{bmatrix}
	\]
}

\dfn{Matrix to Vector Multiplication}{
	Matrix to vector multiplication is the operation of multiplying a matrix by a vector. The result is a new vector that is the result of the matrix-vector multiplication.
	Matrices are represented as $n \times m$, where $n$ is the number of rows and $m$ is the number of columns. The number of columns of the matrix must be equal to the number of rows of the vector.
	\[
		\begin{bmatrix}
			a_{11} & a_{12} & \cdots & a_{1n} \\
			a_{21} & a_{22} & \cdots & a_{2n} \\
			\vdots & \vdots & \ddots & \vdots \\
			a_{n1} & a_{n2} & \cdots & a_{nn}
		\end{bmatrix}
		\begin{bmatrix}
			x_1    \\
			x_2    \\
			\vdots \\
			x_n
		\end{bmatrix}
		=
		\begin{bmatrix}
			a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n \\
			a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n \\
			\vdots                                     \\
			a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n
		\end{bmatrix}
	\]
}

\dfn{Matrix to Matrix Multiplication}{
	Matrix to matrix multiplication is the non-commutative operation of multiplying two matrices together. The result is a new matrix that is the product of the two original matrices.

	For two matrices \(A\) and \(B\) to be multiplied, the number of columns of \(A\) must be equal to the number of rows of \(B\). If \(A\) is an \(m \times n\) matrix and \(B\) is an \(n \times p\) matrix, then their product \(C = AB\) is an \(m \times p\) matrix.

	The element \(c_{ij}\) of the resulting matrix \(C\) is computed as:
	\[
		c_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}
	\]
	where \(a_{ik}\) is the element from the \(i\)-th row and \(k\)-th column of matrix \(A\), and \(b_{kj}\) is the element from the \(k\)-th row and \(j\)-th column of matrix \(B\).

	\[
		\begin{bmatrix}
			a_{11} & a_{12} & \cdots & a_{1n} \\
			a_{21} & a_{22} & \cdots & a_{2n} \\
			\vdots & \vdots & \ddots & \vdots \\
			a_{m1} & a_{m2} & \cdots & a_{mn}
		\end{bmatrix}
		\begin{bmatrix}
			b_{11} & b_{12} & \cdots & b_{1p} \\
			b_{21} & b_{22} & \cdots & b_{2p} \\
			\vdots & \vdots & \ddots & \vdots \\
			b_{n1} & b_{n2} & \cdots & b_{np}
		\end{bmatrix}
		=
		\begin{bmatrix}
			c_{11} & c_{12} & \cdots & c_{1p} \\
			c_{21} & c_{22} & \cdots & c_{2p} \\
			\vdots & \vdots & \ddots & \vdots \\
			c_{m1} & c_{m2} & \cdots & c_{mp}
		\end{bmatrix}
	\]
}

\ex{Matrix to Matrix Multiplication}{
	Consider the matrices
	\[
		A =
		\begin{bmatrix}
			1 & 2 \\
			3 & 4 \\
			5 & 6
		\end{bmatrix}
		\quad \text{and} \quad
		B =
		\begin{bmatrix}
			7  & 8  & 9  \\
			10 & 11 & 12
		\end{bmatrix}.
	\]
	Their product \(C = AB\) is computed as follows:
	\[
		C =
		\begin{bmatrix}
			1 \cdot 7 + 2 \cdot 10 & 1 \cdot 8 + 2 \cdot 11 & 1 \cdot 9 + 2 \cdot 12 \\
			3 \cdot 7 + 4 \cdot 10 & 3 \cdot 8 + 4 \cdot 11 & 3 \cdot 9 + 4 \cdot 12 \\
			5 \cdot 7 + 6 \cdot 10 & 5 \cdot 8 + 6 \cdot 11 & 5 \cdot 9 + 6 \cdot 12
		\end{bmatrix}
		=
		\begin{bmatrix}
			27 & 30  & 33  \\
			61 & 68  & 75  \\
			95 & 106 & 117
		\end{bmatrix}.
	\]
}

\begin{figure}[h]
	\centering
	\begin{tikzpicture}
		% Matrix A
		\matrix[matrix of math nodes,left delimiter={[},right delimiter={]}] (A) {
				a_{11} & a_{12} & \cdots & a_{1n} \\
				a_{21} & a_{22} & \cdots & a_{2n} \\
				\vdots & \vdots & \ddots & \vdots \\
				a_{m1} & a_{m2} & \cdots & a_{mn} \\
			};

		% Matrix B
		\matrix[matrix of math nodes,left delimiter={[},right delimiter={]},right=of A] (B) {
				b_{11} & b_{12} & \cdots & b_{1p} \\
				b_{21} & b_{22} & \cdots & b_{2p} \\
				\vdots & \vdots & \ddots & \vdots \\
				b_{n1} & b_{n2} & \cdots & b_{np} \\
			};

		% Matrix C
		\matrix[matrix of math nodes,left delimiter={[},right delimiter={]},right=of B] (C) {
				c_{11} & c_{12} & \cdots & c_{1p} \\
				c_{21} & c_{22} & \cdots & c_{2p} \\
				\vdots & \vdots & \ddots & \vdots \\
				c_{m1} & c_{m2} & \cdots & c_{mp} \\
			};

		% Labels
		\node[left=0.5cm of A] {$A$};
		\node[right=0.5cm of B] {$B$};
		\node[right=0.5cm of C] {$C = AB$};

		% Arrows for multiplication
		\foreach \i in {1,...,4} {
				\draw[->] (A-\i-1.east) -- (B-1-1.north);
				\draw[->] (A-\i-2.east) -- (B-2-1.north);
				\draw[->] (A-\i-3.east) -- (B-3-1.north);
				\draw[->] (A-\i-4.east) -- (B-4-1.north);
			}

		\foreach \j in {1,...,4} {
				\draw[->] (B-1-\j.south) -- (C-1-\j.west);
				\draw[->] (B-2-\j.south) -- (C-2-\j.west);
				\draw[->] (B-3-\j.south) -- (C-3-\j.west);
				\draw[->] (B-4-\j.south) -- (C-4-\j.west);
			}
	\end{tikzpicture}
	\caption{Matrix-Matrix Multiplication Diagram}
	\label{fig:matrix_multiplication}
\end{figure}

\dfn{Matrix Multiplication of a Matrix with Itself}{
	When a matrix \(A\) is multiplied by its transpose \(A^T\), the resulting matrix is a symmetric matrix. The element \(c_{ij}\) of the resulting matrix \(C = AA^T\) is computed as:
	\[
		c_{ij} = \sum_{k=1}^{n} a_{ik} a_{jk}
	\]
	where \(a_{ik}\) is the element from the \(i\)-th row and \(k\)-th column of matrix \(A\), and \(a_{jk}\) is the element from the \(j\)-th row and \(k\)-th column of matrix \(A\).

	\[
		\begin{bmatrix}
			a_{11} & a_{12} & \cdots & a_{1n} \\
			a_{21} & a_{22} & \cdots & a_{2n} \\
			\vdots & \vdots & \ddots & \vdots \\
			a_{m1} & a_{m2} & \cdots & a_{mn}
		\end{bmatrix}
		\begin{bmatrix}
			a_{11} & a_{21} & \cdots & a_{m1} \\
			a_{12} & a_{22} & \cdots & a_{m2} \\
			\vdots & \vdots & \ddots & \vdots \\
			a_{1n} & a_{2n} & \cdots & a_{mn}
		\end{bmatrix}
		=
		\begin{bmatrix}
			c_{11} & c_{12} & \cdots & c_{1m} \\
			c_{21} & c_{22} & \cdots & c_{2m} \\
			\vdots & \vdots & \ddots & \vdots \\
			c_{m1} & c_{m2} & \cdots & c_{mm}
		\end{bmatrix}
	\]

	The zeros in this matrix represent orthogonality between the corresponding rows of the original matrix \(A\). Specifically, if \(c_{ij} = 0\), it means that the \(i\)-th row and the \(j\)-th row of matrix \(A\) are orthogonal to each other.

}

\ex{Orthogonality in Matrix Multiplication}{
	Consider the matrix
	\[
		A =
		\begin{bmatrix}
			1 & 0 & 0 \\
			0 & 1 & 0 \\
			0 & 0 & 1
		\end{bmatrix}.
	\]
	Its transpose is
	\[
		A^T =
		\begin{bmatrix}
			1 & 0 & 0 \\
			0 & 1 & 0 \\
			0 & 0 & 1
		\end{bmatrix}.
	\]
	The product \(C = AA^T\) is computed as follows:
	\[
		C =
		\begin{bmatrix}
			1 & 0 & 0 \\
			0 & 1 & 0 \\
			0 & 0 & 1
		\end{bmatrix}
		\begin{bmatrix}
			1 & 0 & 0 \\
			0 & 1 & 0 \\
			0 & 0 & 1
		\end{bmatrix}
		=
		\begin{bmatrix}
			1 & 0 & 0 \\
			0 & 1 & 0 \\
			0 & 0 & 1
		\end{bmatrix}.
	\]
	Notice that the resulting matrix is the identity matrix, which is symmetric and has zeros in all off-diagonal elements, indicating that the rows (and columns) of the original matrix \(A\) are orthogonal to each other.
}

\subsection{Interpretation of vectors in $\mathbb{R}^{2,3}$}

\dfn{Position Vector}{
	A position vector is a vector that describes the position of an object in space with reference to an origin.
}
\dfn{Translational Vector}{
	A translational vector is a vector that describes the displacement of an object in space with reference to an origin.
}

\section{Rotation Matrices}

\nt{
	We are considering vectors in $\mathbb{R}^2$, denoted by
	\[
		\vec{v} \;=\;
		\begin{bmatrix}
			x_1 \\
			x_2
		\end{bmatrix}.
	\]
	We also have a $2\times 2$ matrix (linear operator or transformation) given by
	\[
		A
		\;=\;
		\begin{bmatrix}
			1 & 2 \\[6pt]
			0 & 1
		\end{bmatrix}.
	\]
}

\nt{
	Applying $A$ to an input vector $\vec{v}_i$ produces the output vector $\vec{v}_o$:
	\[
		A \,\vec{v}_i \;=\;\vec{v}_o.
	\]
	Explicitly,
	\[
		\begin{bmatrix}
			1 & 2 \\[3pt]
			0 & 1
		\end{bmatrix}
		\begin{bmatrix}
			x_1 \\
			x_2
		\end{bmatrix}
		\;=\;
		\begin{bmatrix}
			y_1 \\[3pt]
			y_2
		\end{bmatrix}.
	\]
}

\qs{Transforming a region}{
	Suppose we restrict the input vectors $\vec{v}_i$ to the square region in the $(x_1,x_2)$-plane with coordinates
	\[
		0 \,\leq\, x_1 \,\leq\, 1,\quad 0 \,\leq\, x_2 \,\leq\, 1.
	\]
	\begin{itemize}
		\item How does $A$ map this square region in the input space to a region in the $(y_1,y_2)$-plane?
		\item Geometrically, what does that image region look like?
	\end{itemize}
}

\sol
We can answer this question \emph{mechanically} by observing what happens to the corners of the square, or by spanning the space with two basis vectors:
\[
	\vec{e}_1 \;=\;\begin{bmatrix}1\\0\end{bmatrix},\quad
	\vec{e}_2 \;=\;\begin{bmatrix}0\\1\end{bmatrix}.
\]
Each point in the square can be written as
\[
	x_1\,\vec{e}_1\;+\;x_2\,\vec{e}_2\quad\text{with }0\le x_1,x_2\le1.
\]

Applying $A$ to these basis vectors, we get
\[
	A\,\vec{e}_1 \;=\;
	\begin{bmatrix}
		1 & 2 \\[3pt]
		0 & 1
	\end{bmatrix}
	\begin{bmatrix}
		1 \\
		0
	\end{bmatrix}
	\;=\;
	\begin{bmatrix}
		1 \\
		0
	\end{bmatrix},
	\quad
	A\,\vec{e}_2 \;=\;
	\begin{bmatrix}
		1 & 2 \\[3pt]
		0 & 1
	\end{bmatrix}
	\begin{bmatrix}
		0 \\
		1
	\end{bmatrix}
	\;=\;
	\begin{bmatrix}
		2 \\
		1
	\end{bmatrix}.
\]

Hence, the image of the unit square (spanned by $\vec{e}_1$ and $\vec{e}_2$) is the parallelogram spanned by
\[
	A\,\vec{e}_1 \;=\; \begin{bmatrix}1\\0\end{bmatrix}
	\quad \text{and} \quad
	A\,\vec{e}_2 \;=\; \begin{bmatrix}2\\1\end{bmatrix}.
\]
In other words, every point $(x_1,x_2)$ in the original square is mapped to
\[
	x_1 \begin{bmatrix}1\\0\end{bmatrix}
	+ x_2 \begin{bmatrix}2\\1\end{bmatrix}
	\;=\;
	\begin{bmatrix}
		x_1 + 2\,x_2 \\
		x_2
	\end{bmatrix},
\]
with $0 \le x_1,x_2 \le 1$. Geometrically, this results in a parallelogram in the $(y_1,y_2)$-plane whose vertices are $(0,0)$, $(1,0)$, $(1+2,1)$, and $(2,1)$.

\nt{
	Thus, restricting $\vec{v}_i$ to a square region in the input space restricts $\vec{v}_o$ to a parallelogram in the output space.
}

\vspace{1cm}
% Example of how an algorithm might appear
\begin{algorithm}[H]
	\KwIn{Input vector $\vec{v}_i = [x_1 \; x_2]^T$ with $0\le x_1,x_2\le1$}
	\KwOut{Output vector $\vec{v}_o = [y_1 \; y_2]^T$ in the parallelogram}
	\SetAlgoLined
	\SetNoFillComment
	\tcc{Matrix $A$:}
	$A \leftarrow \begin{bmatrix}1 & 2 \\ 0 & 1\end{bmatrix}$\;
	\tcc{Apply $A$ to input:}
	$y_1 \leftarrow x_1 + 2\,x_2$\;
	$y_2 \leftarrow x_2$\;
	\Return $\vec{v}_o$\;
	\caption{Mapping a unit square under the linear transformation $A$}
\end{algorithm}

\subsection{Examples of 2D Matrix Transformations}

\ex{
	\(A = \begin{pmatrix} 1 & 0 \\[6pt] 0 & 2 \end{pmatrix}\)
}{
	\textbf{Transformation:} Vertical stretch scaling \(x_2\) by a factor of \(2\).

	\centering
	\begin{tikzpicture}[scale=0.5]
		\draw[->] (-1,0) -- (2,0) node[right] {$x_1$};
		\draw[->] (0,-1) -- (0,3) node[above] {$x_2$};
		\draw[thick, blue, ->] (0,0) -- (1,0) node[below] {$\mathbf{e}_1$};
		\draw[thick, red, ->] (0,0) -- (0,1) node[left] {$\mathbf{e}_2$};
		\draw[thick, red, dashed, ->] (0,0) -- (0,2) node[left] {$A\mathbf{e}_2$};
	\end{tikzpicture}
}

\ex{
	\(A = \begin{pmatrix} -1 & 0 \\[6pt] 0 & 1 \end{pmatrix}\)
}{
	\textbf{Transformation:} Reflection about the vertical axis.
	\centering
	\begin{tikzpicture}[scale=0.5]
		\draw[->] (-2,0) -- (2,0) node[right] {$x_1$};
		\draw[->] (0,-1) -- (0,2) node[above] {$x_2$};
		\draw[thick, blue, ->] (0,0) -- (1,0) node[below] {$\mathbf{e}_1$};
		\draw[thick, red, ->] (0,0) -- (0,1) node[left] {$\mathbf{e}_2$};
		\draw[thick, blue, dashed, ->] (0,0) -- (-1,0) node[below] {$A\mathbf{e}_1$};
	\end{tikzpicture}
}

\ex{
	\(A = \begin{pmatrix} 0 & 1 \\[6pt] 1 & 0 \end{pmatrix}\)
}{
	\textbf{Transformation:} Reflection about the line \(x_1 = x_2\).
	\centering
	\begin{tikzpicture}[scale=0.5]
		\draw[->] (-1,0) -- (2,0) node[right] {$x_1$};
		\draw[->] (0,-1) -- (0,2) node[above] {$x_2$};
		\draw[thick, blue, ->] (0,0) -- (1,0) node[below] {$\mathbf{e}_1$};
		\draw[thick, red, ->] (0,0) -- (0,1) node[left] {$\mathbf{e}_2$};
		\draw[thick, blue, dashed, ->] (0,0) -- (0,1) node[left] {$A\mathbf{e}_1$};
		\draw[thick, red, dashed, ->] (0,0) -- (1,0) node[below] {$A\mathbf{e}_2$};
		\draw[dotted] (-1,-1) -- (2,2);
	\end{tikzpicture}
}

\ex{
	\(A = \begin{pmatrix}
		\cos\theta & -\sin\theta \\[6pt]
		\sin\theta & \cos\theta
	\end{pmatrix}\)
}{
	\textbf{Transformation:} Counterclockwise rotation by \(\theta\) about the origin.
	\centering
	\begin{tikzpicture}[scale=0.5]
		\draw[->] (-1,0) -- (2,0) node[right] {$x_1$};
		\draw[->] (0,-1) -- (0,2) node[above] {$x_2$};
		\def\theta{30}
		\draw[thick, blue, ->] (0,0) -- (1,0) node[below] {$\mathbf{e}_1$};
		\draw[thick, red, ->] (0,0) -- (0,1) node[left] {$\mathbf{e}_2$};
		\draw[thick, blue, dashed, ->] (0,0) -- ({cos(\theta)}, {sin(\theta)}) node[right] {$A\mathbf{e}_1$};
		\draw[thick, red, dashed, ->] (0,0) -- ({-sin(\theta)}, {cos(\theta)}) node[above] {$A\mathbf{e}_2$};
		\draw (0.5,0) arc (0:\theta:0.5) node[midway, right] {$\theta$};
	\end{tikzpicture}
}

\ex{
	\(A = \begin{pmatrix} 0 & 0 \\[6pt] 0 & 1 \end{pmatrix}\)
}{
	\textbf{Transformation:} Projection onto the vertical axis.
	\centering
	\begin{tikzpicture}[scale=0.5]
		\draw[->] (-1,0) -- (2,0) node[right] {$x_1$};
		\draw[->] (0,-1) -- (0,2) node[above] {$x_2$};
		\draw[thick, ->] (0,0) -- (1,1) node[above right] {$\mathbf{v}$};
		\draw[thick, dashed, ->] (0,0) -- (0,1) node[left] {$A\mathbf{v}$};
		\draw[dotted] (1,1) -- (0,1);
	\end{tikzpicture}
}

\section{Mapping}

\[
	[x_1,\; x_2] \;\longmapsto\; \text{point }(x_1, x_2).
\]

Under the linear transformation given by
\[
	[x_1,\; x_2] \;\mapsto\;
	\begin{bmatrix}
		x_1 + 2x_2 \\
		x_2
	\end{bmatrix},
\]
we see that points are ``stretched to the right'' in the \(x_1\)-direction, while the \(x_2\) coordinate remains unchanged.

\subsection*{Method 1 -- Vector Method}

The matrix under consideration is
\[
	A \;=\; \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}.
\]
The columns of this matrix are:
\[
	\begin{bmatrix} 1 \\ 0 \end{bmatrix}
	\quad\text{and}\quad
	\begin{bmatrix} 2 \\ 1 \end{bmatrix}.
\]
These are the images of the standard basis vectors \(\mathbf{e}_1\) and \(\mathbf{e}_2\), respectively.

\begin{center}
	\begin{tikzpicture}[scale=1.4, >=Stealth, baseline={(0,0)}]
		% Axes
		\draw[->] (-0.5,0) -- (3,0) node[right] {\(x_1\)};
		\draw[->] (0,-0.5) -- (0,2) node[above] {\(x_2\)};

		% e1 vector
		\draw[->, very thick, blue] (0,0) -- (1,0) node[midway, below] {\(\mathbf{e}_1\)};

		% e2 vector
		\draw[->, very thick, red] (0,0) -- (2,1) node[midway, above right] {\(\mathbf{e}_2\) mapped to (2,1)};

		% Labels
		\node at (1,0) [below right, blue] {$(1,0)$};
		\node at (2,1) [above right, red] {$(2,1)$};

		\draw (0,0) node[below left] {O};
	\end{tikzpicture}
\end{center}

\subsection*{Method 2 -- Vertex Method}

Consider the unit square with vertices
\[
	(0,0), \quad (1,0), \quad (1,1), \quad (0,1).
\]
We apply \(A\) to each vertex:

\[
	A \begin{bmatrix} 0 \\ 0 \end{bmatrix}
	= \begin{bmatrix} 0 \\ 0 \end{bmatrix},\quad
	A \begin{bmatrix} 1 \\ 0 \end{bmatrix}
	= \begin{bmatrix} 1 \\ 0 \end{bmatrix},\quad
	A \begin{bmatrix} 0 \\ 1 \end{bmatrix}
	= \begin{bmatrix} 2 \\ 1 \end{bmatrix},\quad
	A \begin{bmatrix} 1 \\ 1 \end{bmatrix}
	= \begin{bmatrix} 3 \\ 1 \end{bmatrix}.
\]

Hence the new vertices are \((0,0)\), \((1,0)\), \((2,1)\), and \((3,1)\). Plotting these points yields the transformed parallelogram (the image of the original unit square).

\begin{center}
	\begin{tikzpicture}[scale=1.2, >=Stealth, baseline={(0,0)}]
		% Axes
		\draw[->] (-0.5,0) -- (4,0) node[right] {\(x_1\)};
		\draw[->] (0,-0.5) -- (0,2) node[above] {\(x_2\)};

		% Original square
		\draw[very thin, gray] (0,0) grid (3,1);

		% Original square edges (dashed)
		\draw[dashed] (0,0) -- (1,0) -- (1,1) -- (0,1) -- cycle;

		% Transformed parallelogram
		\draw[ultra thick, blue] (0,0) -- (1,0) -- (3,1) -- (2,1) -- cycle;

		% Points (labels)
		\fill (0,0) circle (2pt) node[below left] {$(0,0)$};
		\fill (1,0) circle (2pt) node[below] {$(1,0)$};
		\fill (2,1) circle (2pt) node[above left] {$(2,1)$};
		\fill (3,1) circle (2pt) node[above right] {$(3,1)$};
	\end{tikzpicture}
\end{center}

\dfn{Rules for Linear Transformations}{

	\begin{enumerate}
		\item \textbf{Straight lines remain straight.}
		\item \textbf{Parallel lines remain parallel.}
		\item \textbf{Distances along lines scale in a consistent, proportional way.}
	\end{enumerate}
}

\subsubsection{Justification}

\paragraph{1. Straight lines stay straight.}
A line in parametric form is:
\[
	r(t) = t\,\mathbf{v} + \mathbf{w}.
\]
Applying \(A\) gives:
\[
	A(r(t)) = A(t\,\mathbf{v} + \mathbf{w}) = t\,A(\mathbf{v}) + A(\mathbf{w}),
\]
which is again a parametric line.

\paragraph{2. Parallel lines stay parallel.}
If two lines are parallel, their direction vectors are scalar multiples of each other. After applying \(A\), the resulting direction vectors are \(A(\mathbf{v})\) for each original direction \(\mathbf{v}\). Since \(A\) is linear, any scalar multiples remain so, preserving parallelism.

\paragraph{3. Distances scale proportionally.}
For two points \(\mathbf{v}_1\) and \(\mathbf{v}_2\), the difference is \(\mathbf{d} = \mathbf{v}_2 - \mathbf{v}_1\). Under \(A\):
\[
	A\mathbf{v}_2 - A\mathbf{v}_1 = A(\mathbf{v}_2 - \mathbf{v}_1) = A(\mathbf{d}).
\]
Thus the new distance \(\|\mathbf{d}'\| = \|A(\mathbf{d})\|\) is a consistent transform of \(\|\mathbf{d}\|\), depending on the nature of \(A\).

\ex{
	A matrix \(A\) that acts on a parallelogram (spanned by two vectors \(\mathbf{x}_1, \mathbf{x}_2\)) will produce another parallelogram in the output plane.
}{
	\begin{center}
		\begin{tikzpicture}[scale=1.1, >=Stealth, baseline={(0,0)}]
			% Original parallelogram
			\coordinate (O) at (0,0);
			\coordinate (X1) at (1,0.3);
			\coordinate (X2) at (0.6,1);

			\draw[->] (O) -- ++(2,0) node[right] {\(x_1\)};
			\draw[->] (O) -- ++(0,2) node[above] {\(x_2\)};

			\filldraw[fill=blue!15, draw=blue!70!black, thick]
			(O) -- (X1) -- ($(X1)+(X2)$) -- (X2) -- cycle;
			\node at ($(X1)!0.5!(O)$) [below, blue] {\(\mathbf{x}_1\)};
			\node at ($(X2)!0.5!(O)$) [left, blue] {\(\mathbf{x}_2\)};

			% Arrow indicating transformation
			\draw[->, very thick] (1.5,1) -- (3,1) node[midway, above] {$A$};

			% Transformed parallelogram
			% Let's define some coordinates for the transformed shape
			\coordinate (O2) at (4,0);               % Shift for the second shape
			\coordinate (X1t) at ($(O2)+(1.4,0.6)$); % Some transformed version of x1
			\coordinate (X2t) at ($(O2)+(0.8,1.2)$); % Some transformed version of x2

			\filldraw[fill=red!15, draw=red!70!black, thick]
			(O2) -- (X1t) -- ($(X1t)+(X2t)-(O2)$) -- (X2t) -- cycle;
		\end{tikzpicture}
	\end{center}

	\noindent
	\emph{Here, lines remain lines, parallels remain parallel.}
}

\section{A System of Linear Equations}

We have a matrix \(A\) of size \(m \times n\), multiplying an unknown vector
\(\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}\)
(which belongs to \(\mathbb{R}^n\)), producing a result vector
\(\begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_m \end{bmatrix}\)
in \(\mathbb{R}^m\):
\[
	A_{m \times n}
	\begin{bmatrix}
		x_1    \\
		x_2    \\
		\vdots \\
		x_n
	\end{bmatrix}
	=
	\begin{bmatrix}
		v_1    \\
		v_2    \\
		\vdots \\
		v_m
	\end{bmatrix}.
\]

This can be viewed as a \emph{system of \(m\) linear equations in \(n\) unknowns}. We are often interested in two main questions:

\begin{enumerate}
	\item \textbf{Does a solution exist?}
	      That is, can we find \(x_1, x_2, \dots, x_n\) so that \(A\mathbf{x} = \mathbf{v}\)?
	      Geometrically, this asks if \(\mathbf{v}\) lies in the \emph{column space} (or image) of \(A\).

	\item \textbf{If at least one solution exists, is it unique or are there infinitely many?}
	      Uniqueness is typically tied to whether the columns of \(A\) are linearly independent (and whether \(m\), \(n\) are related in a way that gives a single solution).  If there are \emph{fewer} pivots than unknowns, or if the system is underdetermined, infinitely many solutions can occur.
\end{enumerate}

\vspace{1em}
\noindent
\textit{Key intuition:}
\begin{itemize}
	\item The question of \emph{existence} boils down to whether \(\mathbf{v}\) is in the span of the columns of \(A\).
	\item The question of \emph{uniqueness} depends on whether those columns form a set of independent vectors and on the relationship between \(m\) and \(n\).
\end{itemize}


\section{How to Determine Consistency, Uniqueness, and the Number of Solutions for a Linear System}
\nt{
	We consider a linear system
	\[
		\begin{cases}
			a_{11} x_1 \;+\; a_{12} x_2 \;+\; \cdots + a_{1n} x_n \;=\; b_1 \\[6pt]
			a_{21} x_1 \;+\; a_{22} x_2 \;+\; \cdots + a_{2n} x_n \;=\; b_2 \\
			\vdots &                                                        \\
			a_{m1} x_1 \;+\; a_{m2} x_2 \;+\; \cdots + a_{mn} x_n \;=\; b_m
		\end{cases}
	\]
	and write it in \emph{augmented matrix} form:
	\[
		\bigl[A \mid \mathbf{b}\bigr]
		\;=\;
		\begin{bmatrix}
			a_{11} & a_{12} & \cdots & a_{1n} & b_1    \\
			a_{21} & a_{22} & \cdots & a_{2n} & b_2    \\
			\vdots & \vdots & \ddots & \vdots & \vdots \\
			a_{m1} & a_{m2} & \cdots & a_{mn} & b_m
		\end{bmatrix}.
	\]

	\begin{itemize}
		\item We perform \emph{elementary row operations} (EROs) on \(\bigl[A \mid \mathbf{b}\bigr]\) to attempt to solve the system. The EROs are:
		      \begin{enumerate}
			      \item \textbf{Scaling (multiplying a row by a nonzero constant).}
			      \item \textbf{Row replacement} (\(R_i \;=\; R_i \;+\; c\,R_j\) for \(i\neq j\)).
			      \item \textbf{Interchanging two rows} (\(R_i \leftrightarrow R_j\)).
		      \end{enumerate}
		      These operations do not change the solution set of the system.
	\end{itemize}

	\textbf{Row Echelon Form (REF)}
	We say \(\bigl[A\mid\mathbf{b}\bigr]\) is in \emph{row echelon form} if:
	\begin{itemize}
		\item All rows of all zeros (if any) are at the bottom of the matrix.
		\item Each \emph{pivot} (leftmost nonzero entry in a nonzero row) is strictly to the right of the pivot in the row above.
	\end{itemize}
	A pivot column in \(A\) corresponds to a \emph{leading variable} (or \emph{basic variable}), and any other column (apart from the augmented column) is called a \emph{free column} (its corresponding variable is a \emph{free variable}).

	\textbf{Consistency \& Number of Solutions}
	\begin{itemize}
		\item If, in the augmented matrix, there is a pivot in the last column (meaning a row of the form
		      \[
			      [\,0\quad 0\quad \cdots\quad 0 \mid c\,],\quad c \neq 0,
		      \]
		      ) then the system is \emph{inconsistent} (no solutions). This corresponds to an equation \(0 = c\) where \(c\neq 0\), which is impossible.
		\item If no such contradiction is found, then at least one solution exists (the system is \emph{consistent}).
		\item The number of pivot columns in \(A\) (i.e.\ the number of leading variables) tells us whether solutions are unique or infinite:
		      \begin{enumerate}
			      \item If the number of pivots equals the number of unknowns \(n\), then there is exactly one solution (assuming no inconsistency).
			      \item If the number of pivots is less than \(n\), then there are free variables, implying infinitely many solutions (again, assuming no inconsistency).
		      \end{enumerate}
	\end{itemize}

	\ex{Example: Augmented Matrix and REF}{
		\[
			\begin{bmatrix}
				1 & 2 & 3 & 5  \\
				2 & 4 & 6 & 9  \\
				3 & 6 & 9 & 15
			\end{bmatrix}
			\;\longrightarrow\;\text{(REF)}.
		\]
		One performs row operations to get an upper-triangular or echelon form.  If the last row becomes something like
		\[
			[\,0\;\;0\;\;0\mid c\,], \quad c\neq 0,
		\]
		then there is no solution.  Otherwise, we identify pivot columns, read off the relationships among variables, and find a general solution (unique or infinite).
	}

	\dfn{Free Columns and Basic Columns}{
		Once an augmented matrix is in REF, we label each pivot column (in \(A\)) as a \emph{basic column}, and any other column (except the last augmented column) as a \emph{free column}.
		\begin{itemize}
			\item If \(x_j\) corresponds to a free column, we may label \(x_j\) as a parameter (e.g.\ \(t_1, t_2, \dots\)).
			\item The variables in pivot columns can then be written in terms of these parameters.
		\end{itemize}
		In this manner, we get a general solution describing the entire solution set to \(A\mathbf{x} = \mathbf{b}\).
	}

	\textbf{Algorithm to Convert \(\bigl[A\mid \mathbf{b}\bigr]\) to \(\mathrm{REF}(A)\)}
	\begin{enumerate}
		\item \emph{Select a candidate row}: Choose the topmost row \emph{among those not yet having a pivot} in which a pivot might appear.
		\item \emph{Pivot search and possible row swap}: Among this candidate row and those below it, find a row having a leftmost nonzero entry in the desired pivot column. Interchange (swap) that row with the candidate row if needed, placing a nonzero entry where your pivot should be.
		\item \emph{Declare the pivot and eliminate below}: Scale the pivot row (if desired) so that the pivot becomes \(1\). Then use row replacement to produce zeros below that pivot in the same column.
		\item Move to the next row down and next column to the right, and repeat until you have a row echelon form.
	\end{enumerate}

	One can then further use row replacement operations to clear the entries \emph{above} each pivot, yielding the \emph{reduced} row echelon form (RREF). However, for most solution purposes, REF is already sufficient to read off whether solutions exist, how many, and so on.
}
\subsection{FRR}
\begin{algorithm}[H]
	\KwIn{Matrix \(A\) of size \(m \times n\) and vector \(\mathbf{b}\) of size \(m\)}
	\KwOut{Upper triangular matrix \(A\) and modified vector \(\mathbf{b}\)}
	\SetAlgoLined
	\SetNoFillComment
	\tcc{Forward elimination process}
	\For{$k \leftarrow 1$ \KwTo $\min(m, n)$}{
		\For{$i \leftarrow k+1$ \KwTo $m$}{
			\If{$A_{kk} \neq 0$}{
				$f \leftarrow A_{ik} / A_{kk}$\;
				\For{$j \leftarrow k$ \KwTo $n$}{
					$A_{ij} \leftarrow A_{ij} - f \cdot A_{kj}$\;
				}
				$b_i \leftarrow b_i - f \cdot b_k$\;
			}
		}
	}
	\Return $A, \mathbf{b}$\;
	\caption{Forward Row Reduction (Forward Elimination)}
\end{algorithm}

\section{Back Row Reduction}

\nt{
	Back row reduction, also known as back substitution, is a method used to solve a system of linear equations that has been transformed into an upper triangular form through Gaussian elimination. This method involves solving the equations starting from the last row and moving upwards.
}

\dfn{Back Row Reduction}{
	Consider a system of linear equations represented in matrix form as \(A\mathbf{x} = \mathbf{b}\), where \(A\) is an upper triangular matrix:
	\[
		\begin{bmatrix}
			a_{11} & a_{12} & \cdots & a_{1n} \\
			0      & a_{22} & \cdots & a_{2n} \\
			\vdots & \vdots & \ddots & \vdots \\
			0      & 0      & \cdots & a_{nn}
		\end{bmatrix}
		\begin{bmatrix}
			x_1    \\
			x_2    \\
			\vdots \\
			x_n
		\end{bmatrix}
		=
		\begin{bmatrix}
			b_1    \\
			b_2    \\
			\vdots \\
			b_n
		\end{bmatrix}.
	\]
	The solution is obtained by solving the last equation first and then substituting the obtained values into the preceding equations.
}

\ex{Example of Back Row Reduction}{
	Consider the following upper triangular system:
	\[
		\begin{cases}
			2x_1 + 3x_2 + 4x_3 = 5, \\
			0x_1 + 6x_2 + 7x_3 = 8, \\
			0x_1 + 0x_2 + 9x_3 = 10.
		\end{cases}
	\]
	We start with the last equation:
	\[
		9x_3 = 10 \quad \Rightarrow \quad x_3 = \frac{10}{9}.
	\]
	Next, we substitute \(x_3\) into the second equation:
	\[
		6x_2 + 7\left(\frac{10}{9}\right) = 8 \quad \Rightarrow \quad 6x_2 + \frac{70}{9} = 8 \quad \Rightarrow \quad 6x_2 = 8 - \frac{70}{9} \quad \Rightarrow \quad x_2 = \frac{2}{9}.
	\]
	Finally, we substitute \(x_2\) and \(x_3\) into the first equation:
	\[
		2x_1 + 3\left(\frac{2}{9}\right) + 4\left(\frac{10}{9}\right) = 5 \quad \Rightarrow \quad 2x_1 + \frac{6}{9} + \frac{40}{9} = 5 \quad \Rightarrow \quad 2x_1 = 5 - \frac{46}{9} \quad \Rightarrow \quad x_1 = \frac{1}{9}.
	\]
	Thus, the solution is:
	\[
		x_1 = \frac{1}{9}, \quad x_2 = \frac{2}{9}, \quad x_3 = \frac{10}{9}.
	\]
}

\begin{algorithm}[H]
	\KwIn{Upper triangular matrix \(A\) of size \(n \times n\) and vector \(\mathbf{b}\) of size \(n\)}
	\KwOut{Solution vector \(\mathbf{x}\) of size \(n\)}
	\SetAlgoLined
	\SetNoFillComment
	\tcc{Initialize solution vector}
	\For{$i \leftarrow n$ \KwTo $1$}{
		$x_i \leftarrow b_i$\;
		\For{$j \leftarrow i+1$ \KwTo $n$}{
			$x_i \leftarrow x_i - A_{ij} \cdot x_j$\;
		}
		$x_i \leftarrow x_i / A_{ii}$\;
	}
	\Return $\mathbf{x}$\;
	\caption{Back Row Reduction (Back Substitution)}
\end{algorithm}

\ex{Full SLE solving}{
	$$ \begin{bmatrix}
			1 & 1  & 2 & 2  \\
			1 & -1 & 0 & -2 \\
			2 & 0  & m & 0
		\end{bmatrix}
	$$
	$$
		\begin{bmatrix}
			1 & 1  & 2   & 2  \\
			0 & -2 & -2  & -4 \\
			0 & -2 & m-4 & -4
		\end{bmatrix}
	$$
	$$
		\begin{bmatrix}
			1 & 1  & 2   & 2  \\
			0 & -2 & -2  & -4 \\
			0 & 0  & m-2 & 0
		\end{bmatrix}
	$$
	given m=0
	$$
		\begin{bmatrix}
			1 & 1  & 2  & 2  \\
			0 & -2 & -2 & -4 \\
			0 & 0  & 0  & 0
		\end{bmatrix}
	$$
	$$
		\begin{bmatrix}
			1 & 1 & 2 & 2 \\
			0 & 1 & 1 & 2 \\
			0 & 0 & 0 & 0
		\end{bmatrix}
	$$
	$$
		\begin{bmatrix}
			1 & 0 & 1 & 0 \\
			0 & 1 & 1 & 2 \\
			0 & 0 & 0 & 0
		\end{bmatrix}
	$$

}


\end{document}